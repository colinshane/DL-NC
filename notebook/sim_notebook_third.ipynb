{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForword NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "'''This helps in predicting no. of transaction when a customer has 'x' no. of children and\n",
    " 'y' no. of bank accounts for four customers.'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#input_data = [#_of_children, #_of_accounts] for 4 customers\n",
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "def relu(input):\n",
    "    output = max(0, input)\n",
    "    return output\n",
    "\n",
    "def predict_with_network(input_data_row, weights):\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "    \n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "    \n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    return model_output\n",
    "\n",
    "results = []\n",
    "\n",
    "for input_data_row in input_data:\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient_Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZGY+/9G8e4pUnWtswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x101aea828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pred(input_data, target, weights):\n",
    "    return ((input_data * weights).sum())\n",
    "\n",
    "def get_slope(input_data, target, weights):\n",
    "    preds = pred(input_data, target, weights)\n",
    "    error = target - preds\n",
    "    slope = 2 * input_data * error\n",
    "    return slope\n",
    "\n",
    "def get_mse(input_data, target, weights):\n",
    "    preds = pred(input_data, target, weights)\n",
    "    return mean_squared_error([preds], [target])\n",
    "\n",
    "learning_rate = 0.01\n",
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0\n",
    "\n",
    "n_updates = 20\n",
    "mse_hist = []\n",
    "for i in range(n_updates):\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    weights = weights + (learning_rate * slope)\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    mse_hist.append(mse)\n",
    "    \n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient_Descent_orange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def problem(x):\n",
    "    e = 2.71828182845904590\n",
    "    return x[0]**5 + e**x[1] + x[0]**3 + x[0] + x[1] - 5\n",
    "\n",
    "def error(x):\n",
    "    return (problem(x)-0)**2\n",
    "\n",
    "def gradient_descent(x):\n",
    "    delta = 0.00000001\n",
    "\n",
    "    derivative_x0 = (error([x[0] + delta, x[1]]) - error([x[0] - delta, x[1]])) / (delta * 2)\n",
    "    derivative_x1 = (error([x[0], x[1] + delta]) - error([x[0], x[1] - delta])) / (delta * 2)\n",
    "\n",
    "    alpha = 0.01\n",
    "    x[0] = x[0] - derivative_x0 * alpha\n",
    "    x[1] = x[1] - derivative_x1 * alpha\n",
    "    return [x[0],x[1]]\n",
    "\n",
    "x = [0.0, 0.0]\n",
    "for i in range(50):\n",
    "    x = gradient_descent(x)\n",
    "    print('x = {:6f},{:6f}, problem(x) = {:6f}'.format(x[0],x[1],problem(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic_Gradient_Descent_With_LearningRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 5.4229     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 5.3936     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 5.3643     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 5.3347     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 5.3049     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 5.2750     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 5.2450     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 5.2147     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 5.1843     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 5.1539     \n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 1.5531     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 0.7078     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.6745     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6216     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.6177     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.6125     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.6065     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.6135     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5856     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.5895     \n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 6.0960     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 6.1867     \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 9 12:05:43 2017\n",
    "@author: neelabhpant\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "'''Cleaning Training Data'''\n",
    "\n",
    "train_data = pd.read_csv('../Brian2_scripts/Data/neelabhpant/train.csv')\n",
    "train_survived = [i for i in train_data['Survived']]\n",
    "train_pclass = [i for i in train_data['Pclass']]\n",
    "train_age = [29.699118 if math.isnan(i) else i for i in train_data['Age']]\n",
    "train_age_was_missing = [1 if math.isnan(i) else 0 for i in train_data['Age']]\n",
    "train_sibsp = [i for i in train_data['SibSp']]\n",
    "train_parch = [i for i in train_data['Parch']]\n",
    "train_fare = [i for i in train_data['Fare']]\n",
    "train_male = [1 if i == 'male' else 0 for i in train_data['Sex']]\n",
    "train_embarked_from_cherbourg = [1 if i == 'C' else 0 for i in train_data['Embarked']]\n",
    "train_embarked_from_queenstown = [1 if i == 'Q' else 0 for i in train_data['Embarked']]\n",
    "train_embarked_from_southampton = [1 if i == 'S' else 0 for i in train_data['Embarked']]\n",
    "train_dict = {'survived': train_survived,\n",
    "              'pclass': train_pclass,\n",
    "              'age': train_age,\n",
    "              'age_was_missing': train_age_was_missing,\n",
    "              'sibsp': train_sibsp,\n",
    "              'parch': train_parch,\n",
    "              'fare': train_fare,\n",
    "              'male': train_male,\n",
    "              'embarked_from_cherbourg': train_embarked_from_cherbourg,\n",
    "              'embarked_from_queenstown': train_embarked_from_queenstown,\n",
    "              'embarked_from_southampton': train_embarked_from_southampton}\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "train_predictors = train_df.as_matrix(columns=train_df.columns[:10])\n",
    "train_n_cols = train_predictors.shape[1]\n",
    "train_target = to_categorical(train_df.survived)\n",
    "\n",
    "'''Cleaning Testing Data'''\n",
    "\n",
    "test_data = pd.read_csv('../Brian2_scripts/Data/neelabhpant/test.csv')\n",
    "test_pclass = [i for i in test_data['Pclass']]\n",
    "test_age = [29.699118 if math.isnan(i) else i for i in test_data['Age']]\n",
    "test_age_was_missing = [1 if math.isnan(i) else 0 for i in test_data['Age']]\n",
    "test_sibsp = [i for i in test_data['SibSp']]\n",
    "test_parch = [i for i in test_data['Parch']]\n",
    "test_fare = [i for i in test_data['Fare']]\n",
    "test_male = [1 if i == 'male' else 0 for i in test_data['Sex']]\n",
    "test_embarked_from_cherbourg = [1 if i == 'C' else 0 for i in test_data['Embarked']]\n",
    "test_embarked_from_queenstown = [1 if i == 'Q' else 0 for i in test_data['Embarked']]\n",
    "test_embarked_from_southampton = [1 if i == 'S' else 0 for i in test_data['Embarked']]\n",
    "test_dict = {'pclass': test_pclass,\n",
    "             'age': test_age,\n",
    "             'age_was_missing': test_age_was_missing,\n",
    "             'sibsp': test_sibsp,\n",
    "             'parch': test_parch,\n",
    "             'fare': test_fare,\n",
    "             'male': test_male,\n",
    "             'embarked_from_cherbourg': test_embarked_from_cherbourg,\n",
    "             'embarked_from_queenstown': test_embarked_from_queenstown,\n",
    "             'embarked_from_southampton': test_embarked_from_southampton}\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "test_predictors = test_df.as_matrix()\n",
    "\n",
    "\n",
    "def get_new_model(x):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape=(x,)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return (model)\n",
    "\n",
    "\n",
    "# Different Learning Rates to test the model\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "# Checking the model on different Learning Rates with Stochastic Gradient Descent as Optimizer\n",
    "\n",
    "for lr in lr_to_test:\n",
    "    print(\"\\n\\nTesting model with learning rate: %f\\n\" % lr)\n",
    "    model = get_new_model(train_n_cols)\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    model.fit(train_predictors, train_target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresison_Using_Keras_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 0s - loss: 29.9051     \n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s - loss: 23.3382     \n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s - loss: 22.0726     \n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s - loss: 21.3969     \n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s - loss: 21.3325     \n",
      "Epoch 6/10\n",
      "534/534 [==============================] - ETA: 0s - loss: 20.48 - 0s - loss: 21.1304     \n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s - loss: 21.0013     \n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s - loss: 20.8457     \n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s - loss: 20.8263     \n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s - loss: 20.7288     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1154a7d68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_data(csv_file):\n",
    "    cps = pd.read_csv(csv_file, header=1)\n",
    "    cps = cps.drop('race', 1)\n",
    "    cps = cps.drop('hispanic', 1)\n",
    "\n",
    "    # Cleaning the data\n",
    "\n",
    "    wage_per_hour = [round(i, 2) for i in cps['wage']]\n",
    "    union = []\n",
    "    education_yrs = [i for i in cps['educ']]\n",
    "    experience_yrs = [i for i in cps['exper']]\n",
    "    age = [i for i in cps['age']]\n",
    "    female = []\n",
    "    marr = []\n",
    "    south = []\n",
    "    manufacturing = []\n",
    "    construction = []\n",
    "\n",
    "    for i in cps['union']:\n",
    "        if i == 'Not':\n",
    "            union.append(0)\n",
    "        else:\n",
    "            union.append(1)\n",
    "\n",
    "    for sex in cps['sex']:\n",
    "        if sex == 'M':\n",
    "            female.append(0)\n",
    "        else:\n",
    "            female.append(1)\n",
    "\n",
    "    for marriage in cps['married']:\n",
    "        if marriage == 'Single':\n",
    "            marr.append(0)\n",
    "        else:\n",
    "            marr.append(1)\n",
    "\n",
    "    for i in cps['south']:\n",
    "        if i == 'NS':\n",
    "            south.append(0)\n",
    "        else:\n",
    "            south.append(1)\n",
    "\n",
    "    for i in cps['sector']:\n",
    "        if i == 'manuf':\n",
    "            manufacturing.append(1)\n",
    "        else:\n",
    "            manufacturing.append(0)\n",
    "\n",
    "    for i in cps['sector']:\n",
    "        if i == 'const':\n",
    "            construction.append(1)\n",
    "        else:\n",
    "            construction.append(0)\n",
    "\n",
    "    # Creating a dictionary for the final DataFrame\n",
    "\n",
    "    my_dict = {'wage_per_hour': wage_per_hour,\n",
    "               'union': union,\n",
    "               'education_yrs': education_yrs,\n",
    "               'experience_yrs': experience_yrs,\n",
    "               'age': age,\n",
    "               'female': female,\n",
    "               'marr': marr,\n",
    "               'south': south,\n",
    "               'manufacturing': manufacturing,\n",
    "               'construction': construction}\n",
    "\n",
    "    # Final DataFrame\n",
    "\n",
    "    df = pd.DataFrame(my_dict)\n",
    "\n",
    "    # Creating Predictors and Targets labels\n",
    "    predictors = df.as_matrix(columns=df.columns[:9])\n",
    "    targets = [i for i in df['wage_per_hour']]\n",
    "    target = np.array(targets)\n",
    "\n",
    "    return [df, predictors, target]\n",
    "\n",
    "\n",
    "[df, predictors, target] = get_data('../Brian2_scripts/Data/neelabhpant/cps.csv')  # Getting Data, Predictors and Target\n",
    "import keras  # i Importing required keras packages\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "n_cols = predictors.shape[1]  # Setting up dimensions of an input\n",
    "model = Sequential()  # Setting up a Sequential model\n",
    "\n",
    "# We have 2 hidden layers\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))  # Setting up 1st Dense layers where each node is connected to every node in the next layer\n",
    "model.add(Dense(32, activation='relu'))  # Setting up 2nd layer\n",
    "\n",
    "# 1 output layer\n",
    "model.add(Dense(1))  # Setting up final layer\n",
    "\n",
    "# Compiling the model using 'adam' optimizer and MSE as loss function\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "print(\"Loss function: \" + model.loss)\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification_Using_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 0s - loss: 0.6270 - acc: 0.6655     \n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s - loss: 0.5991 - acc: 0.6902     \n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s - loss: 0.6047 - acc: 0.6734     \n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s - loss: 0.6002 - acc: 0.6824     \n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s - loss: 0.5913 - acc: 0.6970     \n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s - loss: 0.5949 - acc: 0.6790     \n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s - loss: 0.5801 - acc: 0.7071     \n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s - loss: 0.5872 - acc: 0.6958     \n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s - loss: 0.5640 - acc: 0.7239     \n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s - loss: 0.5806 - acc: 0.6947     \n",
      "[ 0.2425399   0.18980798  0.19601329  0.27374804  0.41992944  0.36463469\n",
      "  0.31861785  0.51819539  0.38264361  0.48163822  0.24400973  0.37650728\n",
      "  0.62684643  0.43545693  0.68404943  0.53042662  0.35197932  0.29760784\n",
      "  0.30524689  0.2277583   0.54592896  0.36262208  0.54634112  0.58524698\n",
      "  0.56229764  0.34613872  0.5968225   0.29007006  0.3781299   0.37842792\n",
      "  0.38754517  0.53483707  0.41991293  0.41038287  0.51184225  0.3127192\n",
      "  0.31044987  0.36449337  0.30238277  0.53322202  0.40181467  0.44594803\n",
      "  0.19741604  0.4129056   0.61957967  0.26156336  0.37063569  0.26549059\n",
      "  0.7107715   0.47346699  0.52994502  0.38788784  0.51046169  0.69728142\n",
      "  0.3970803   0.30203819  0.22092953  0.25389645  0.37561512  0.65160877\n",
      "  0.30617946  0.3561196   0.29564765  0.36609104  0.77275884  0.40638369\n",
      "  0.39396411  0.44342771  0.44748774  0.61439067  0.35678059  0.28048187\n",
      "  0.30620205  0.46813318  0.59577465  0.60528612  0.24863142  0.42812559\n",
      "  0.35073465  0.35678059  0.4941664   0.69805568  0.38398537  0.24400973\n",
      "  0.33502135  0.39512265  0.34376612  0.36701518  0.32933372  0.57858443\n",
      "  0.41161707  0.24038206  0.54165399  0.24863142  0.49074727  0.25797433\n",
      "  0.53514189  0.24505328  0.35106924  0.23321882  0.65956509  0.45162439\n",
      "  0.26549059  0.25302941  0.44303444  0.39701921  0.30822659  0.26549059\n",
      "  0.26822373  0.36696172  0.40065476  0.33028558  0.44749647  0.37759119\n",
      "  0.69713736  0.39988503  0.22971424  0.67341399  0.56031799  0.44320264\n",
      "  0.51981884  0.26024318  0.59013456  0.22939897  0.26549059  0.47131553\n",
      "  0.2724092   0.41733634  0.33427039  0.26943207  0.23706612  0.39372379\n",
      "  0.43654293  0.22381529  0.19168404  0.264117    0.2680673   0.35585323\n",
      "  0.34053892  0.57099622  0.3223739   0.60368818  0.61532772  0.44851935\n",
      "  0.36530536  0.36088729  0.55955344  0.27905953  0.44594803  0.40959758\n",
      "  0.63962209  0.27537039         nan  0.43121222  0.34243649  0.25573659\n",
      "  0.66907883  0.33507377  0.36530536  0.44438562  0.3287887   0.48755836\n",
      "  0.41154447  0.21717669  0.33642498  0.42727047  0.52897549  0.44830397\n",
      "  0.47737744  0.36700231  0.23362032  0.26547644  0.36312905  0.2544128\n",
      "  0.35415894  0.53756559  0.53210896  0.50075334  0.54949111  0.72451282\n",
      "  0.35073465  0.56107187  0.54959542  0.26549059  0.70455986  0.3844023\n",
      "  0.49073476  0.30808416  0.31741336  0.33839247  0.37323239  0.4374176\n",
      "  0.49485329  0.26078099  0.51099628  0.227349    0.7377457   0.36106107\n",
      "  0.32303691  0.31250238  0.44773781  0.70945626  0.57772011  0.50144082\n",
      "  0.31656063  0.43253484  0.29387236  0.31966186  0.43493614  0.26075447\n",
      "  0.37628704  0.21866129  0.57012767  0.464553    0.24469422  0.50480062\n",
      "  0.33353075  0.67691028  0.62160641  0.24863142  0.43219426  0.28441679\n",
      "  0.39123216  0.27790287  0.43958932  0.47591981  0.26730785  0.32933372\n",
      "  0.38282898  0.34455341  0.55155665  0.57201791  0.240683    0.26937628\n",
      "  0.54369634  0.28536075  0.64123791  0.30317646  0.42687428  0.58089876\n",
      "  0.44591513  0.39330196  0.66881639  0.24376066  0.38459817  0.63778079\n",
      "  0.47256324  0.36842248  0.44320264  0.44721094  0.62547702  0.28714186\n",
      "  0.57264227  0.30185714  0.2778998   0.23362032  0.26549059  0.25265089\n",
      "  0.4126569   0.27736396  0.38926893  0.27941161  0.41874158  0.71572977\n",
      "  0.37941518  0.24400973  0.09759552  0.23362032  0.31044987  0.32221755\n",
      "  0.61381865  0.26549059  0.57832128  0.43974891  0.25428072  0.54912913\n",
      "  0.30826956  0.35754728  0.36252525  0.32676071  0.3567318   0.6888454\n",
      "  0.32933372  0.5188933   0.62288284  0.19823723  0.22462483  0.61761349\n",
      "  0.2544128   0.24863142  0.6228295   0.29639891  0.2544128   0.66485822\n",
      "  0.26564085  0.25644222  0.57296568  0.37842792  0.60811788  0.24298055\n",
      "  0.22885096  0.39682323  0.41248816  0.28553969  0.32933372  0.47463563\n",
      "  0.57482505  0.72455388  0.59366602  0.40000197  0.31532764  0.29249021\n",
      "  0.23437157  0.28268597  0.62317413  0.3988933   0.63035101  0.33913255\n",
      "  0.25107875  0.5226531   0.25302941  0.27615559  0.35585323  0.40173337\n",
      "  0.58677202  0.24683662  0.46299717  0.58410722  0.42365918  0.35768786\n",
      "  0.38860789  0.38575974  0.25428072  0.45500037  0.25226885  0.42719537\n",
      "  0.34879473  0.23085971  0.57268155  0.2544128   0.34410065  0.22302216\n",
      "  0.54037023  0.73098791  0.38544589  0.37361476  0.35585323  0.26248011\n",
      "  0.35826361  0.4046151   0.69664639  0.31656063  0.58053416  0.68546999\n",
      "  0.66696692  0.38634431  0.41990846  0.2435115   0.26549059  0.4202826\n",
      "  0.27084804  0.56362164  0.4046151   0.27374804  0.52099502  0.54672045\n",
      "  0.39515135  0.53358996  0.68869823  0.38898236  0.33942398  0.61778659\n",
      "  0.06103706  0.32939216  0.68937379  0.58437747  0.34152159  0.34587207\n",
      "  0.44644865  0.35051712  0.26549059  0.28315267  0.43444434  0.43499991\n",
      "  0.35155192  0.5683549   0.26194829  0.28339529  0.30658114  0.43229672\n",
      "  0.60976636  0.3970879   0.51409221  0.24719352  0.36249262  0.57807308\n",
      "  0.27632299  0.61334813  0.27185929  0.25520095  0.5951671   0.37425515\n",
      "  0.58673412  0.54626721  0.36164522  0.38538191  0.31519267  0.61899686\n",
      "  0.32838005  0.6561358   0.32933372  0.58625633  0.30647102  0.24863142\n",
      "  0.64964163  0.18933244  0.24863143  0.38855675]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 7 16:15:43 2017\n",
    "@author: neelabhpant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "'''Cleaning Training Data'''\n",
    "\n",
    "train_data = pd.read_csv('../Brian2_scripts/Data/neelabhpant/train.csv')\n",
    "train_survived = [i for i in train_data['Survived']]\n",
    "train_pclass = [i for i in train_data['Pclass']]\n",
    "train_age = [29.699118 if math.isnan(i) else i for i in train_data['Age']]\n",
    "train_age_was_missing = [1 if math.isnan(i) else 0 for i in train_data['Age']]\n",
    "train_sibsp = [i for i in train_data['SibSp']]\n",
    "train_parch = [i for i in train_data['Parch']]\n",
    "train_fare = [i for i in train_data['Fare']]\n",
    "train_male = [1 if i=='male' else 0 for i in train_data['Sex']]\n",
    "train_embarked_from_cherbourg = [1 if i=='C' else 0 for i in train_data['Embarked']]\n",
    "train_embarked_from_queenstown = [1 if i=='Q' else 0 for i in train_data['Embarked']]\n",
    "train_embarked_from_southampton = [1 if i=='S' else 0 for i in train_data['Embarked']]\n",
    "train_dict = {'survived':train_survived,\n",
    "              'pclass':train_pclass,\n",
    "              'age':train_age,\n",
    "              'age_was_missing':train_age_was_missing,\n",
    "              'sibsp':train_sibsp,\n",
    "              'parch':train_parch,\n",
    "              'fare':train_fare,\n",
    "              'male':train_male,\n",
    "              'embarked_from_cherbourg':train_embarked_from_cherbourg,\n",
    "              'embarked_from_queenstown':train_embarked_from_queenstown,\n",
    "              'embarked_from_southampton':train_embarked_from_southampton}\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "train_predictors = train_df.as_matrix(columns=train_df.columns[:10])\n",
    "train_n_cols = train_predictors.shape[1]\n",
    "train_target = to_categorical(train_df.survived)\n",
    "\n",
    "'''Setting up the model'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(train_n_cols,)))\n",
    "model.add(Dense(100, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_predictors, train_target)\n",
    "\n",
    "\n",
    "'''Cleaning Testing Data'''\n",
    "\n",
    "test_data = pd.read_csv('../Brian2_scripts/Data/neelabhpant/test.csv')\n",
    "test_pclass = [i for i in test_data['Pclass']]\n",
    "test_age = [29.699118 if math.isnan(i) else i for i in test_data['Age']]\n",
    "test_age_was_missing = [1 if math.isnan(i) else 0 for i in test_data['Age']]\n",
    "test_sibsp = [i for i in test_data['SibSp']]\n",
    "test_parch = [i for i in test_data['Parch']]\n",
    "test_fare = [i for i in test_data['Fare']]\n",
    "test_male = [1 if i=='male' else 0 for i in test_data['Sex']]\n",
    "test_embarked_from_cherbourg = [1 if i=='C' else 0 for i in test_data['Embarked']]\n",
    "test_embarked_from_queenstown = [1 if i=='Q' else 0 for i in test_data['Embarked']]\n",
    "test_embarked_from_southampton = [1 if i=='S' else 0 for i in test_data['Embarked']]\n",
    "test_dict = {'pclass':test_pclass,\n",
    "              'age':test_age,\n",
    "              'age_was_missing':test_age_was_missing,\n",
    "              'sibsp':test_sibsp,\n",
    "              'parch':test_parch,\n",
    "              'fare':test_fare,\n",
    "              'male':test_male,\n",
    "              'embarked_from_cherbourg':test_embarked_from_cherbourg,\n",
    "              'embarked_from_queenstown':test_embarked_from_queenstown,\n",
    "              'embarked_from_southampton':test_embarked_from_southampton}\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "test_predictors = test_df.as_matrix()\n",
    "\n",
    "\n",
    "'''Making the predictions'''\n",
    "\n",
    "predictions = model.predict(test_predictors)\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "print(predicted_prob_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
